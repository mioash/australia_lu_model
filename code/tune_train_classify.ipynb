{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ee'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-11f2ac094ac6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mee\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mee\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Initialize the Earth Engine object, using the authentication credentials.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mee\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ee'"
     ]
    }
   ],
   "source": [
    "import ee \n",
    "from ee import batch\n",
    "\n",
    "# Initialize the Earth Engine object, using the authentication credentials.\n",
    "ee.Initialize()\n",
    "from make_clim import make_climate\n",
    "\n",
    "from stack_images_qgis import stacking\n",
    "from mask_image import define_image\n",
    "from stratitication import make_strata\n",
    "from stratitication import prop_allocation\n",
    "from change_no_change import suit_layer \n",
    "\n",
    "from add_neigh import add_neigh\n",
    "from add_distance import add_distance\n",
    "#from change_no_change import suit_layer \n",
    "import geemap\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "val_year = 2015\n",
    "train_year = 2010\n",
    "i_year = 1983\n",
    "#lc_img = define_image(50,0,'tmov',train_year,5) #temporal mov threshold >= 40, \n",
    "lc_img_classify = define_image(0,0,'none',train_year,5)\n",
    "lc_img_for_masking = define_image(50,0,'tmov',train_year,5)\n",
    "\n",
    "lim_temp_f = ee.FeatureCollection('users/mioash/aust_cd66states')\n",
    "ucl = ee.FeatureCollection('users/mioash/SUA_2016_AUST')\n",
    "vic = lim_temp_f.filterMetadata('STE', 'equals', 6)\n",
    "#geom = vic\n",
    "geom = lim_temp_f\n",
    "\n",
    "roads = ee.FeatureCollection('users/verdemuskuna/aus_roads/gis_osm_roads_free_1')\n",
    "wways = ee.FeatureCollection('users/verdemuskuna/aus_roads/gis_osm_waterways_free_1')\n",
    "pop = ee.ImageCollection(\"projects/sat-io/open-datasets/hrsl/hrslpop\").median().clip(geom)\n",
    "nat_npa = ee.Image(\"users/mioash/drivers_lcaus/natural_areas/nat_npa_ready\")\n",
    "tas_dist_neigh = ee.Image(\"users/mioash/Calderon_etal_Australian_land-cover/Tas_neigh_dists_c2005\")\n",
    "\n",
    "proj = lc_img_classify.projection()\n",
    "#################CHANGE ACCESSIBILITY\n",
    "\n",
    "fst_img = stacking(train_year-10,train_year-5,'mean',False,False) # initial year, end year, reducer for climate, get climate, get precipitation\n",
    "bfst = fst_img.bandNames().getInfo()\n",
    "\n",
    "#clm_mean = stacking(train_year-10,train_year-5,'p50',True,True)\n",
    "# clm_lterm = stacking(i_year,train_year-5,'p50',True,True)\n",
    "# clm_anomaly = clm_mean.subtract(clm_lterm)\n",
    "\n",
    "# bn = clm_anomaly.bandNames().getInfo()\n",
    "# bn = [\"{}{}\".format(i,str('_anom')) for i in bn]\n",
    "# clm_anomaly = clm_anomaly.rename(bn) \n",
    "\n",
    "\n",
    "roads = roads.filter(ee.Filter.inList('fclass', ['motorway','trunk','primary','secondary','unclassified','tertiary','residential','living_street']))\n",
    "rd = ee.Image(0).paint(roads,'code').updateMask(fst_img.select('cindex'))\n",
    "rd = rd.updateMask(rd.neq(0))\n",
    "dist_roads = rd.select('constant').fastDistanceTransform(1000).sqrt().multiply(ee.Image.pixelArea().sqrt())#//.divide(1000000)//.clip(to_clip)\n",
    "dist_roads = dist_roads.rename('dist_roads').updateMask(fst_img.select('cindex').add(100))\n",
    "\n",
    "#dist_roads = dist_roads.expression(\n",
    "#  '(1 + (1/dd))',{\n",
    "#  'dd': dist_roads.select(['dist_roads'])\n",
    "#}).rename('dist_roads')\n",
    "\n",
    "\n",
    "wways = wways.filter(ee.Filter.inList('fclass', ['river']))\n",
    "ww = ee.Image(0).paint(wways,'code').updateMask(fst_img.select('cindex'))\n",
    "ww = ww.updateMask(ww.neq(0))\n",
    "dist_wways = ww.select('constant').fastDistanceTransform(1000).sqrt().multiply(ee.Image.pixelArea().sqrt())#//.divide(1000000)//.clip(to_clip)\n",
    "dist_wways = dist_wways.rename('dist_wways').updateMask(fst_img.select('cindex').add(100))\n",
    "\n",
    "pop = pop.unmask(0).reproject(proj.atScale(30))#.clip(geom)\n",
    "pop = pop.select('b1').rename('pop')\n",
    "\n",
    "img_for_train = lc_img_classify.addBands(fst_img).addBands(dist_roads).addBands(dist_wways).addBands(pop)\n",
    "\n",
    "lcclasses = [0,1,2,3,4]\n",
    "cpixels = [15,15,15,15,15]\n",
    "\n",
    "#imgn = img_for_train\n",
    "idist = img_for_train.select('lc_t1').rename('lc_temp')\n",
    "\n",
    "for i in range(0,len(lcclasses)):\n",
    "    idist = add_distance(idist,lcclasses[i],'lc_temp',geom,cpixels[i])\n",
    "\n",
    "idist = idist.select(idist.bandNames().remove(str('lc_t1')).getInfo())\n",
    "idist = idist.select(idist.bandNames().remove(str('lc_temp')).getInfo())\n",
    "\n",
    "nat_npa = nat_npa.select('npa')\n",
    "to_test = ee.Image(0).clip(geom).where(nat_npa,1)\n",
    "to_test = to_test.rename('npa')\n",
    "\n",
    "distto_npa = add_distance(to_test,1,'npa',geom,1)\n",
    "distto_npa = distto_npa.rename(['npa','distto_npa'])\n",
    "\n",
    "\n",
    "img_for_train = img_for_train.addBands(distto_npa.select('distto_npa')).addBands(idist)\n",
    "#add_neigh(image,size,size_centre,blank):\n",
    "\n",
    "ineigh = ee.Image(1).addBands(add_neigh(lc_img_classify.select('lc_t1'),3,1,False,'_nb')) #-> r=1\n",
    "ineigh = ineigh.addBands(add_neigh(lc_img_classify.select('lc_t1'),9,3,True,'_3b')) #-> r=4\n",
    "#ineigh = ineigh.addBands(add_neigh(lc_img.select('lc_t1'),9,1,False,'_nb')) #-> r=4\n",
    "\n",
    "ineigh = ineigh.addBands(add_neigh(lc_img_classify.select('lc_t1'),27,9,True,'_9b')) #-> r=13\n",
    "#ineigh = ineigh.addBands(add_neigh(lc_img.select('lc_t1'),27,1,False,'_nb')) #-> r=13\n",
    "\n",
    "ineigh = ineigh.addBands(add_neigh(lc_img_classify.select('lc_t1'),81,27,True,'_81b')) #-> r=33\n",
    "#ineigh = ineigh.addBands(add_neigh(lc_img.select('lc_t1'),67,1,False,'_nb')) #-> r=33\n",
    "\n",
    "img_for_train = img_for_train.addBands(ineigh)\n",
    "# img_for_train = img_for_train.addBands(add_neigh(lc_img.select('lc_t1'),3,1,False,'_nb')) #-> r=1\n",
    "\n",
    "# img_for_train = img_for_train.addBands(add_neigh(lc_img.select('lc_t1'),9,3,True,'_3b')) #-> r=4\n",
    "# img_for_train = img_for_train.addBands(add_neigh(lc_img.select('lc_t1'),9,1,False,'_nb')) #-> r=4\n",
    "\n",
    "# img_for_train = img_for_train.addBands(add_neigh(lc_img.select('lc_t1'),27,9,True,'_9b')) #-> r=13\n",
    "# img_for_train = img_for_train.addBands(add_neigh(lc_img.select('lc_t1'),27,1,False,'_nb')) #-> r=13\n",
    "\n",
    "# img_for_train = img_for_train.addBands(add_neigh(lc_img.select('lc_t1'),67,27,True,'_27b')) #-> r=33\n",
    "# img_for_train = img_for_train.addBands(add_neigh(lc_img.select('lc_t1'),67,1,False,'_nb')) #-> r=33\n",
    "\n",
    "\n",
    "im_to_exp = idist.addBands(distto_npa.select('distto_npa')).addBands(ineigh).addBands(dist_roads).addBands(dist_wways)\n",
    "\n",
    "img_ready = lc_img_classify.addBands(fst_img).addBands(tas_dist_neigh).addBands(pop)\n",
    "img_readya = img_ready.updateMask(lc_img_for_masking.add(1).select('lc_dep'))\n",
    "img_readya = img_readya.unmask(-9999,False).updateMask(lc_img_classify.add(ee.Image(1)).select('lc_t1'))\n",
    "\n",
    "bbands = img_ready.bandNames().getInfo()\n",
    "#print(bbands)\n",
    "\n",
    "def myproperties(feature):\n",
    "  feature=ee.Feature(feature).setGeometry(None)\n",
    "  return feature\n",
    "\n",
    "def LatLonImg(img,bands_to_get,npts,binary):\n",
    "   \n",
    "    img = img.addBands(ee.Image.pixelLonLat())\n",
    "    if binary is True:\n",
    "        strat = make_strata(img,2,True)\n",
    "    else:\n",
    "        strat = make_strata(img,6,False)\n",
    "    ssamples = prop_allocation(ee.Image(strat).toInt(),geom,npts)\n",
    "    # ssamples = img.sampleRegions(collection= ssamples, \n",
    "    #                              scale= 30,\n",
    "    #                              tileScale=2)\n",
    "    \n",
    "    ssamples = img.reduceRegion(reducer=ee.Reducer.toList(),\\\n",
    "                                geometry = ssamples, \\\n",
    "                                 scale= 30,\\\n",
    "                                 tileScale=2)\n",
    "    \n",
    "    #tvals = tvals.map(myproperties)\n",
    "    #ssamples = ssamples.map(myproperties)\n",
    "    #print(ssamples.getInfo())\n",
    "    # img = img.reduceRegion(reducer=ee.Reducer.toList(),\\\n",
    "    #                                     geometry=region,\\\n",
    "    #                                     maxPixels=1e13,\\\n",
    "    #                                     scale=30,\\\n",
    "    #                                     tileScale= 2);\n",
    " \n",
    "    #sa2id = np.array((ee.Array(img.get(\"sa2_id\")).getInfo()))#.astype('i4')\n",
    "    lats = np.array((ee.Array(ssamples.get(\"latitude\")).getInfo()))#.astype('i4')\n",
    "    lons = np.array((ee.Array(ssamples.get(\"longitude\")).getInfo()))#.astype('i4')\n",
    "    arr = np.vstack([lats, lons])\n",
    "    \n",
    "    for i in range(0,len(bands_to_get)):\n",
    "         bx = np.array((ee.Array(ssamples.get(str(bands_to_get[i]))).getInfo()))\n",
    "         arr = np.vstack([arr,bx])\n",
    "    #arr = arr\n",
    "    # cr = np.array((ee.Array(img.get(\"prob_crop\")).getInfo()))#.astype('i4')\n",
    "    # fr = np.array((ee.Array(img.get(\"prob_forest\")).getInfo()))#.astype('i4')\n",
    "    # gr= np.array((ee.Array(img.get(\"prob_grass\")).getInfo()))#.astype('i4')\n",
    "    # ur= np.array((ee.Array(img.get(\"prob_urban\")).getInfo()))#.astype('i4')\n",
    "    # ot= np.array((ee.Array(img.get(\"prob_other\")).getInfo()))#.astype('i4')\n",
    "    # lprev= np.array((ee.Array(img.get(\"b2005\")).getInfo()))#.astype('i4')\n",
    "    # lpred= np.array((ee.Array(img.get(\"b2010\")).getInfo()))#.astype('i4')\n",
    "    #arr = np.vstack([lats, lons, sa2id, cr, fr, gr, ur, ot, lprev, lpred]).transpose()\n",
    "    return arr.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-78869e71f291>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-78869e71f291>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip install earthengine-api\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install earthengine-api\n",
    "#conda update -c conda-forge earthengine-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ineigh.bandNames().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geometri = ee.Geometry.Rectangle([143.537,-43.89,148.965,-39.42]) #//#tas\n",
    "\n",
    "# Export image\n",
    "llx = 108.76 \n",
    "lly = -44 \n",
    "urx = 155 \n",
    "ury = -10  #australia\n",
    "geometri = [[llx,lly], [llx,ury], [urx,ury], [urx,lly]]\n",
    "\n",
    "ee.batch.Export.image.toAsset(image=classified1,\n",
    "                              description='Tas_lc2010_tunnedv1_n67_n27_n9'+lcclass,\n",
    "                              assetId='users/mioash/Calderon_etal_Australian_land-cover/Tas_lc2010_tunnedv1_n67_n27_n9'+lcclass,scale=30,\n",
    "                             region = geometri,maxPixels=1e13,crs='EPSG:4326').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lc_cnc_c = suit_layer(lc_img_classify,'binary',True,0)#c\n",
    "lc_cnc_f = suit_layer(lc_img_classify,'binary',True,1)#f\n",
    "lc_cnc_g = suit_layer(lc_img_classify,'binary',True,2)#g\n",
    "lc_cnc_u = suit_layer(lc_img_classify,'binary',True,3)#u\n",
    "#lc_cnc_w = suit_layer(lc_img_classify,'binary',True,4)#w\n",
    "#lc_cnc_o = suit_layer(lc_img_classify,'binary',True,5)#o\n",
    "\n",
    "Map = geemap.Map(center=[-37.74,144.93], zoom=7)\n",
    "\n",
    "Map.add_basemap(\"SATELLITE\")\n",
    "\n",
    "#Map.addLayer(ee.Image(strat).clip(geom), {'bands':['strata'],'min': 100, 'max': 410, 'opacity':1}, 'strat')\n",
    "Landcover_vispar = {'bands':['lc_dep'],\"opacity\":1,\"min\":0,\"max\":1,\"palette\":[\"FFEE88\",\"882255\"]}\n",
    "#Map.addLayer(img_for_train, Landcover_vispar, 'Dep')\n",
    "#Map.addLayer(lc_cnc.clip(geom), '', 'cnc')\n",
    "\n",
    "Map.addLayer(lc_cnc_c, Landcover_vispar, 'Dep')\n",
    "\n",
    "Map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_imgb = lc_cnc_u\n",
    "lcclass = 'urban'\n",
    "\n",
    "band_multi = img_ready.bandNames().getInfo()\n",
    "#band_multi = band_multi.remove(['lc_dep', 'lc_t1', 'lc_t2', 'lc_t3', 'lc_t4', 'lc_t5'])\n",
    "item_list = [e for e in band_multi if e not in ('lc_dep', 'lc_t1', 'lc_t2', 'lc_t3', 'lc_t4', 'lc_t5')]\n",
    "lc_imgb = lc_imgb.addBands(img_ready.select(item_list)) \n",
    "\n",
    "lc_imgba = lc_imgb.updateMask(lc_img_for_masking.add(1).select('lc_dep'))\n",
    "lc_imgba = lc_imgb.unmask(-9999,False).updateMask(lc_img_classify.add(ee.Image(1)).select('lc_t1'))\n",
    "print(lc_imgb.bandNames().getInfo())\n",
    "Map.addLayer(lc_imgb, {'min':0,'max':1}, 'Binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_multi = LatLonImg(img_readya, bbands,15000, False)\n",
    "pts_binary = LatLonImg(lc_imgba, bbands,20000, True)\n",
    "\n",
    "pts_multi = pts_multi[pts_multi[:,2] != -9999,:]\n",
    "pts_binary = pts_binary[pts_binary[:,2] != -9999,:]\n",
    "print(np.shape(pts_multi))\n",
    "print(np.shape(pts_binary))\n",
    "#pts = pts[pts[:,2] != 4,:]\n",
    "#pts = pts[pts[:,2] != 5,:]\n",
    "#print(np.shape(pts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pts)\n",
    "#Using Pearson Correlation\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(40,40))\n",
    "pts = pts_binary\n",
    "\n",
    "nbands = ['latitude','longitude']+bbands#,'lc_dep','lc_t1','lc_t2','lc_t3','lc_t4','lc_t5','constant']#+bbands\n",
    "\n",
    "pts_mx = pd.DataFrame(data=pts, columns=nbands)\n",
    "#pts_mx.to_csv(path_or_buf='tas_1st_test.csv',index=False)\n",
    "\n",
    "###find correlated variables\n",
    "corr_matrix = pts_mx.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "labels = np.array(pts_mx['lc_dep'])\n",
    "print(to_drop)\n",
    "\n",
    "# Remove the labels from the features\n",
    "# axis 1 refers to the columns\n",
    "cor = pts_mx.corr()\n",
    "sns.heatmap(cor, cmap=plt.cm.Reds)\n",
    "plt.show()\n",
    "\n",
    "pts_mx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_use_multi = ['c_9_nb', 'f_9_nb', 'g_9_nb', 'b_9_nb', 'w_9_nb', 'o_9_nb',\n",
    "                  'c_27_nb', 'f_27_nb', 'g_27_nb', 'b_27_nb', 'w_27_nb', 'o_27_nb',\n",
    "            'c_67_nb', 'f_67_nb', 'g_67_nb', 'b_67_nb', 'w_67_nb', 'o_67_nb',\n",
    "            'tenure2018','constant','lc_dep','latitude','longitude','aspect','landforms',\n",
    "            'DES_000_200_EV','PTO','AWC','c_67_27b', 'f_67_27b', 'g_67_27b', 'b_67_27b', 'w_67_27b', 'o_67_27b',\n",
    "            ##highly_correlated\n",
    "            'BDW', 'CLY', 'SLT', 'SND', 'PH', 'AWC', 'NTO', 'PTO','p05', 'p08', 'p10', 'p11',  'p16',\n",
    "            'p17', 'p18', 'p19',]\n",
    "\n",
    "dont_use_crop = ['c_9_nb', 'f_9_nb', 'g_9_nb', 'b_9_nb', 'w_9_nb', 'o_9_nb',\n",
    "            'c_27_nb', 'f_27_nb', 'g_27_nb', 'b_27_nb', 'w_27_nb', 'o_27_nb',\n",
    "            'c_67_nb', 'f_67_nb', 'g_67_nb', 'b_67_nb', 'w_67_nb', 'o_67_nb',\n",
    "            'tenure2018','constant','lc_dep','latitude','longitude',\n",
    "                ##low_importance\n",
    "            'p14','p16', 'p17', 'p18', 'p19', 'BDW', 'SND', 'AWC', 'NTO', 'PTO','DES_000_200_EV','aspect','landforms',\n",
    "                 #correlated\n",
    "                'p05', 'p11', 'p13']\n",
    "\n",
    "dont_use_forest = ['c_9_nb', 'f_9_nb', 'g_9_nb', 'b_9_nb', 'w_9_nb', 'o_9_nb',\n",
    "            'c_27_nb', 'f_27_nb', 'g_27_nb', 'b_27_nb', 'w_27_nb', 'o_27_nb',\n",
    "            'c_67_nb', 'f_67_nb', 'g_67_nb', 'b_67_nb', 'w_67_nb', 'o_67_nb',\n",
    "            'tenure2018','constant','lc_dep','latitude','longitude',\n",
    "                ##low_importance\n",
    "            'p16', 'p17', 'p18', 'p19','SOC','CLY','PH', 'BDW', 'SND', 'AWC', 'NTO', 'PTO','DES_000_200_EV',\n",
    "                   'aspect','landforms',]\n",
    "                 #correlated\n",
    "            #    'p05', 'p11', 'p13']\n",
    "\n",
    "        \n",
    "dont_use_urban = ['c_9_nb', 'f_9_nb', 'g_9_nb', 'b_9_nb', 'w_9_nb', 'o_9_nb',\n",
    "                  'c_9_3b', 'f_9_3b', 'g_9_3b', 'b_9_3b', 'w_9_3b', 'o_9_3b',\n",
    "            'c_27_nb', 'f_27_9b', 'g_27_9b', 'b_27_9b', 'w_27_9b', 'o_27_9b',\n",
    "            'c_27_9b', 'f_27_nb', 'g_27_nb', 'b_27_nb', 'w_27_nb', 'o_27_nb',\n",
    "            'c_67_nb', 'f_67_nb', 'g_67_nb', 'b_67_nb', 'w_67_nb', 'o_67_nb',\n",
    "            'c_67_27b', 'f_67_27b', 'g_67_27b', 'b_67_27b', 'w_67_27b', 'o_67_27b',\n",
    "            'tenure2018','constant','lc_dep','latitude','longitude',\n",
    "                ##low_importance\n",
    "            'tenure2013','SOC','CLY','PH', 'BDW', 'NTO', 'PTO','DES_000_200_EV',\n",
    "                   'aspect','landforms','AWC','distto_npa','dist_wways','p09','p14','p13','p16', 'p17', 'p18', 'p19',\n",
    "                 'distto_0','distto_2','distto_4','SLT']\n",
    "           # 'p16', 'p17', 'p18', 'p19','S\n",
    "                           \n",
    "dont_use_grass = ['c_9_nb', 'f_9_nb', 'g_9_nb', 'b_9_nb', 'w_9_nb', 'o_9_nb',\n",
    "            'c_27_nb', 'f_27_nb', 'g_27_nb', 'b_27_nb', 'w_27_nb', 'o_27_nb',\n",
    "            'c_67_nb', 'f_67_nb', 'g_67_nb', 'b_67_nb', 'w_67_nb', 'o_67_nb',\n",
    "            'tenure2018','constant','lc_dep','latitude','longitude',\n",
    "                ##low_importance\n",
    "            'SOC','CLY', 'BDW', 'NTO', 'SLT', 'PTO','DES_000_200_EV','elevation','slope','cindex',\n",
    "                    'aspect','landforms','AWC','distto_npa','dist_wways','pop',\n",
    "                  'p08','p12','p13','p09','p14','p16', 'p18','SND','dist_roads','distto_0','distto_4','distto_3']\n",
    "#                  'distto_0','distto_2','distto_4','SLT']\n",
    "#            # 'p16', 'p17', 'p18', 'p19','S\n",
    "                           \n",
    "\n",
    "        \n",
    "dont_use = dont_use_urban\n",
    "\n",
    "features= pts_mx.drop(dont_use, axis = 1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "print(feature_list)\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "acc = np.zeros((10,2))\n",
    "\n",
    "for i in range(4,5):\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = i/10,\n",
    "                                                                                random_state = 55)\n",
    "    # Instantiate model \n",
    "#     rf = RandomForestClassifier(n_estimators= 100, random_state=55, max_depth= 30, \n",
    "#                                 min_samples_leaf= 4,min_samples_split= 2)\n",
    "    rf = RandomForestClassifier(n_estimators= 100, random_state=55)\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_features, train_labels)\n",
    "    \n",
    "    y_pred = rf.predict(test_features)\n",
    "    \n",
    "    acco = int(accuracy_score(test_labels, y_pred)*1000)\n",
    "    acc[i-1,0] = acco\n",
    "    acc[i-1,1] = np.shape(train_features)[0]\n",
    "    \n",
    "    #print(confusion_matrix(test_labels,y_pred))\n",
    "    #print(classification_report(test_labels,y_pred))\n",
    "    #print(accuracy_score(test_labels, y_pred))\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import cross_val_score\n",
    "#scores = cross_val_score(rf, train_features, train_labels, cv=10)\n",
    "\n",
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 3)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# y_pred_class = y_pred_pos > threshold\n",
    "# cm = confusion_matrix(y_true, y_pred_class)\n",
    "# tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# roc_auc = roc_auc_score(test_labels, y_pred)\n",
    "# from scikitplot.metrics import plot_roc\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# plot_roc(test_labels, y_pred, ax=ax)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#y_pred_class = y_pred_pos > threshold\n",
    "cm = confusion_matrix(test_labels, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "tn\n",
    "#fp\n",
    "#fn\n",
    "#tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##random search\n",
    "from sklearn.model_selection import RandomizedSearchCV# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 500, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [10, 50, 100, 150 , 200, 500]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rfa = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rfa, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = 16)# Fit the random search model\n",
    "rf_random.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    #errors = abs(predictions - test_labels)\n",
    "    #mape = 100 * np.mean(errors / test_labels)\n",
    "    #accuracy = 100 - mape\n",
    "    #rrmse = np.sqrt(metrics.mean_squared_error(test_labels, predictions))\n",
    "    #nrmse = rrmse / np.mean(test_labels)\n",
    "    acco = accuracy_score(test_labels, y_pred)\n",
    "    print('Model Performance')\n",
    "    print(acco)\n",
    "    #print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    #print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    #print('RMSE: ', rrmse)\n",
    "    #print('NRMSE: ', nrmse)    \n",
    "    return RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, test_features, test_labels)\n",
    "\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [30, 10, 20, 40],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [1, 3, 4, 5, 10, 50, 250,300],\n",
    "    'min_samples_split': [1, 2, 3, 10, 50],\n",
    "    'n_estimators': [100, 200]\n",
    "}# Create a based model\n",
    "rfb = RandomForestClassifier()# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rfb, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = 32, verbose = 2)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "grid_search.best_params_\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_\n",
    "#feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lc_change.bandNames().getInfo())\n",
    "#feature_list\n",
    "\n",
    "# trainingvals1 = lc_imgb.sampleRegions(\n",
    "#                             collection= ssamples, \n",
    "#                             scale= 30,\n",
    "#                             tileScale=8)\n",
    "\n",
    "strat = make_strata(lc_imgba,2,True)\n",
    "#strat = make_strata(img,6,False)\n",
    "ssamples = prop_allocation(lc_imgba.addBands(ee.Image(strat).toInt()),geom,10000)\n",
    "    \n",
    "mtry = round(len(feature_list)*(2/3),0)\n",
    "#print(mtry)\n",
    "classifier1 = ee.Classifier.smileRandomForest(numberOfTrees=100,\n",
    "                                               variablesPerSplit =mtry ,\n",
    "                                              minLeafPopulation=4,\n",
    "                                             maxNodes = 40, seed = 55).setOutputMode('PROBABILITY')\n",
    "#classifier2 = ee.Classifier.libsvm('Voting','C_SVC','RBF',True,None,0.5)\n",
    "\n",
    "#Train the classifier.\n",
    "trainer1 = classifier1.train(features=ssamples,classProperty ='lc_dep',\n",
    "                            inputProperties= feature_list)\n",
    "#trainer2 = classifier2.train(trainingvals,'lc_dep')\n",
    "\n",
    "classified1 = lc_imgb.classify(trainer1).toDouble()\n",
    "\n",
    "geometri = ee.Geometry.Rectangle([143.537,-43.89,148.965,-39.42]) #//#tas\n",
    "\n",
    "ee.batch.Export.image.toAsset(image=classified1,\n",
    "                              description='Tas_lc2010_tunnedv1_n67_n27_n9'+lcclass,\n",
    "                              assetId='users/mioash/Calderon_etal_Australian_land-cover/Tas_lc2010_tunnedv1_n67_n27_n9'+lcclass,scale=30,\n",
    "                             region = geometri,maxPixels=1e13,crs='EPSG:4326').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ssamples.size().getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geemap import ml\n",
    "\n",
    "trees =  ml.rf_to_strings(rf,feature_list)\n",
    "\n",
    "user_id = geemap.ee_user_id()\n",
    "user_id\n",
    "\n",
    "ml.export_trees_to_fc(trees,user_id + \"/rf_crop_tas_v1\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
